<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Project Title</title>
  <link rel="stylesheet" href="css/style.css" />
  <script src="script.js" defer></script>
</head>
<body>
  <div class="wrapper">
    <h1 class="title">UniVST: A Unified Framework for Training-free Localized Video Style Transfer</h1>

    <div class="authors">
      <a href="https://github.com/QuanjianSong">Quanjian Song<sup>1</sup></a>,
      <a href="https://scholar.google.com/citations?user=Dp3L1bsAAAAJ&hl=zh-CN&oi=ao">Mingbao Lin<sup>2</sup></a>,
      <a href="https://scholar.google.com/citations?hl=zh-CN&user=nnF1s7kAAAAJ">Wengyi Zhan<sup>1</sup></a>,
      <a href="https://scholar.google.com/citations?user=DNuiPHwAAAAJ&hl=zh-CN&oi=ao">Shuicheng Yan<sup>2</sup></a>,
      <a href="https://mac.xmu.edu.cn/ljcao/">Liujuan Cao<sup>1</sup></a>,
      <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji<sup>1</sup></a>
    </div>

    <div class="affiliations">
      <span><sup>1</sup>Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, China.</span>,
      <span><sup>2</sup>Kunlun Skywork AI</span>
    </div>

    <div class="buttons">
      <a class="btn" href="#">üìù arXiv</a>
      <a class="btn" href="#">üìÑ Supplementary</a>
      <a class="btn" href="#">üíª Code</a>
      <a class="btn" href="#">ü§ñ Online Demo</a>
    </div>

    <p class="tldr"><strong>TL;DR:</strong> We propose StyleCrafter, a generic method that enhances pre-trained T2V models with style control, supporting Style-Guided Text-to-Image Generation and Style-Guided Text-to-Video Generation.</p>
  

  <div class="wrapper">
    <section class="section gray">
        <h2 class="section-title">Abstract</h2>
        <p class="section-text">
          Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos.
          However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity.
          To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image.
          <br><br>
          Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm.
          To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy.
          <br><br>
          Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations.
          StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images.
          Experiments demonstrate that our approach is more flexible and efficient than existing competitors.
        </p>
      </section>
  </div>

</body>



</html>
